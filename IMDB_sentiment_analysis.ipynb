{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['test', 'train', 'unsupervised'])\n"
     ]
    }
   ],
   "source": [
    "print(datasets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='imdb_reviews',\n",
       "    version=1.0.0,\n",
       "    description='Large Movie Review Dataset.\n",
       "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
       "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
       "    features=FeaturesDict({\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "        'text': Text(shape=(), dtype=tf.string),\n",
       "    }),\n",
       "    total_num_examples=100000,\n",
       "    splits={\n",
       "        'test': 25000,\n",
       "        'train': 25000,\n",
       "        'unsupervised': 50000,\n",
       "    },\n",
       "    supervised_keys=('text', 'label'),\n",
       "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
       "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
       "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
       "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
       "      month     = {June},\n",
       "      year      = {2011},\n",
       "      address   = {Portland, Oregon, USA},\n",
       "      publisher = {Association for Computational Linguistics},\n",
       "      pages     = {142--150},\n",
       "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
       "    }\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.splits[\"train\"].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000\n"
     ]
    }
   ],
   "source": [
    "train_size = info.splits[\"train\"].num_examples\n",
    "test_size = info.splits[\"test\"].num_examples\n",
    "print(train_size , test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n",
      "Label: 0 = Negative\n",
      "\n",
      "Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n",
      "Label: 0 = Negative\n",
      "\n",
      "Review: Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Moun ...\n",
      "Label: 0 = Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# traversing through the batches and showing first 200 characters of review and label of the first batch data samples:\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(3).take(1):\n",
    "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
    "        print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
    "        print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 53), dtype=string, numpy=\n",
       " array([[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie',\n",
       "         b\"Don't\", b'be', b'lured', b'in', b'by', b'Christopher',\n",
       "         b'Walken', b'or', b'Michael', b'Ironside', b'Both', b'are',\n",
       "         b'great', b'actors', b'but', b'this', b'must', b'simply', b'be',\n",
       "         b'their', b'worst', b'role', b'in', b'history', b'Even',\n",
       "         b'their', b'great', b'acting', b'could', b'not', b'redeem',\n",
       "         b'this', b\"movie's\", b'ridiculous', b'storyline', b'This',\n",
       "         b'movie', b'is', b'an', b'early', b'nineties', b'US',\n",
       "         b'propaganda', b'pi', b'<pad>', b'<pad>', b'<pad>'],\n",
       "        [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep',\n",
       "         b'during', b'films', b'but', b'this', b'is', b'usually', b'due',\n",
       "         b'to', b'a', b'combination', b'of', b'things', b'including',\n",
       "         b'really', b'tired', b'being', b'warm', b'and', b'comfortable',\n",
       "         b'on', b'the', b'sette', b'and', b'having', b'just', b'eaten',\n",
       "         b'a', b'lot', b'However', b'on', b'this', b'occasion', b'I',\n",
       "         b'fell', b'asleep', b'because', b'the', b'film', b'was',\n",
       "         b'rubbish', b'The', b'plot', b'development', b'was', b'constant',\n",
       "         b'Cons'],\n",
       "        [b'Mann', b'photographs', b'the', b'Alberta', b'Rocky',\n",
       "         b'Mountains', b'in', b'a', b'superb', b'fashion', b'and',\n",
       "         b'Jimmy', b'Stewart', b'and', b'Walter', b'Brennan', b'give',\n",
       "         b'enjoyable', b'performances', b'as', b'they', b'always',\n",
       "         b'seem', b'to', b'do', b'But', b'come', b'on', b'Hollywood',\n",
       "         b'a', b'Mountie', b'telling', b'the', b'people', b'of',\n",
       "         b'Dawson', b'City', b'Yukon', b'to', b'elect', b'themselves',\n",
       "         b'a', b'marshal', b'yes', b'a', b'marshal', b'and', b'to', b'e',\n",
       "         b'<pad>', b'<pad>', b'<pad>', b'<pad>']], dtype=object)>,\n",
       " <tf.Tensor: shape=(3,), dtype=int64, numpy=array([0, 0, 0])>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Truncate the reviews, keeping only the first 300 characters of each since you can generally tell whether a review is positive or not in the first sentence or two.\n",
    "Then we use regular expressions to replace <br/> tags with spaces and to replace any characters other than letters and quotes with spaces.\n",
    "Finally, the preprocess() function splits the reviews by the spaces, which returns a ragged tensor(variable length tensor), \n",
    "and it converts this ragged tensor to a dense tensor(store values in a contiguous sequential block of memory), \n",
    "padding all reviews with the padding token <pad> so that they all have the same length.\n",
    "'''\n",
    "# input Tensor => X_batch\n",
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch\n",
    "\n",
    "preprocess(X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 63155),\n",
       " (b'the', 61137),\n",
       " (b'a', 38564),\n",
       " (b'of', 33983),\n",
       " (b'and', 33431)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "vocabulary = Counter()\n",
    "\n",
    "#make a vocabulary dictionary containing the words and their counts correspondingly\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(2).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))\n",
    "\n",
    "vocabulary.most_common()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.Counter"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore = [b'the',b'a',b'if',b'in',b'it',b'of',b'or',b'the',b'and',b'to']\n",
    "for word in list(vocabulary):\n",
    "    if word in ignore:\n",
    "        del vocabulary[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting the most frequent stop words\n",
    "vocabulary = Counter({k:vocabulary[k] for k in vocabulary if vocabulary[k] < 5665})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'you', 5500),\n",
       " (b'an', 5183),\n",
       " (b'at', 4814),\n",
       " (b'about', 4799),\n",
       " (b'by', 4717),\n",
       " (b'all', 4655),\n",
       " (b'his', 4625),\n",
       " (b'so', 4521),\n",
       " (b'like', 4428),\n",
       " (b'from', 4321),\n",
       " (b'who', 4266),\n",
       " (b'has', 4178),\n",
       " (b'It', 4038),\n",
       " (b'good', 3727),\n",
       " (b'my', 3662),\n",
       " (b'just', 3636),\n",
       " (b'very', 3571),\n",
       " (b'out', 3376),\n",
       " (b'story', 3211),\n",
       " (b'some', 3197)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.most_common()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53864\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Truncating the Vocabulary - keep only top 10k\n",
    "vocab_size = 10000\n",
    "truncated_vocabulary = [ word for word, count in vocabulary.most_common()[:vocab_size]]\n",
    "\n",
    "#Creating a lookup table\n",
    "#Create a tensor words containing the words of truncated_vocabulary\n",
    "words  = tf.constant(truncated_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[10745, 10345, 10637, 10053]])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Computer can only process numbers but not words. Thus we need to convert the words in truncated_vocabulary into numbers.\n",
    "So we now need to add a preprocessing step to replace each word with its ID (i.e., its index in the truncated_vocabulary).\n",
    "We will create a lookup table for this, using 1,000 out-of-vocabulary (oov) buckets.\n",
    "We shall create the lookup table such that the most frequently occurring words have lower indices than less frequently\n",
    "occurring words.'''\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\n",
    "table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "#Apply the function preprocess on every batch of data with 32 samples repeatedly on the train data datasets[\"train\"]\n",
    "train_set = datasets[\"train\"].repeat().batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)\n",
    "test_set = datasets[\"test\"].batch(1000).map(preprocess)\n",
    "test_set = test_set.map(encode_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[10745 10637     1 ... 10403 10403 10403]\n",
      " [10180 10850    42 ... 10403 10403 10403]\n",
      " [ 4070  6852 10354 ... 10403 10403 10403]\n",
      " ...\n",
      " [10745 10345    89 ...   302  1018 10403]\n",
      " [ 1728  4072   422 ... 10403 10403 10403]\n",
      " [ 3336  4363 10180 ... 10403 10403 10403]], shape=(32, 60), dtype=int64)\n",
      "tf.Tensor([0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in train_set.take(1):\n",
    "    print(X_batch)\n",
    "    print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the Model\n",
    "'''The first layer is an Embedding layer, which will convert word IDs into embeddings. \n",
    "The embedding matrix needs to have one row per word ID (vocab_size + num_oov_buckets) and one column per embedding \n",
    "dimension (this example uses 128 dimensions, but this is a hyperparameter you could tune).\n",
    "Whereas the inputs of the model will be 2D tensors of shape [batch size, time steps], the output of the \n",
    "Embedding layer will be a 3D tensor of shape [batch size, time steps, embedding size].'''\n",
    "\n",
    "'''Create the model model with Embedding layer,GRU layer with 4 units\n",
    "GRU layer with 2 units,Dense layer with 1 unit and sigmoid activation'''\n",
    "embed_size = 128\n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "               mask_zero=True,\n",
    "               input_shape=[None]),\n",
    "    keras.layers.GRU(4, return_sequences=True),\n",
    "    keras.layers.GRU(2),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 781 steps\n",
      "Epoch 1/2\n",
      "781/781 [==============================] - 83s 106ms/step - loss: 0.6693 - accuracy: 0.5488\n",
      "Epoch 2/2\n",
      "781/781 [==============================] - 73s 93ms/step - loss: 0.4444 - accuracy: 0.7988\n",
      "Time of execution: 155.60411405563354\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start  = time.time()\n",
    "model.fit(train_set, steps_per_epoch=train_size // 32, epochs=2)\n",
    "end  = time.time()\n",
    "print(\"Time of execution:\", end-start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     25/Unknown - 5s 189ms/step - loss: 0.5195 - accuracy: 0.7641"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5194795954227448, 0.76408]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.173173  ],\n",
       "       [0.61271054],\n",
       "       [0.05875764],\n",
       "       ...,\n",
       "       [0.04878023],\n",
       "       [0.90365326],\n",
       "       [0.882887  ]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Remember, we mapped the positive outputs to 1 and the negative outputs to 0. \n",
    "However, the sigmoid function predicts floating value between 0 and 1. \n",
    "If the value is less than 0.5, the sentiment is considered negative where as if the value is greater than 0.5, \n",
    "the sentiment is considered as positive.'''\n",
    "classes = [0 if i <0.5  else 1 for i in y_pred]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING ...\n",
      "Label: 1 = Positive\n",
      "Predicted Label: [0.173173] Predicted Class 0\n",
      "\n",
      "Review: A blackly comic tale of a down-trodden priest, Nazarin showcases the economy that Luis Bunuel was able to achieve in being able to tell a deeply humanist fable with a minimum of fuss. As an output fro ...\n",
      "Label: 1 = Positive\n",
      "Predicted Label: [0.61271054] Predicted Class 1\n",
      "\n",
      "Review: Scary Movie 1-4, Epic Movie, Date Movie, Meet the Spartans, Not another Teen Movie and Another Gay Movie. Making \"Superhero Movie\" the eleventh in a series that single handily ruined the parody genre. ...\n",
      "Label: 0 = Negative\n",
      "Predicted Label: [0.05875764] Predicted Class 0\n",
      "\n",
      "Review: Poor Shirley MacLaine tries hard to lend some gravitas to this mawkish, gag-inducing \"feel-good\" movie, but she's trampled by the run-away sentimentality of a film that's not the least bit grounded in ...\n",
      "Label: 0 = Negative\n",
      "Predicted Label: [0.11321057] Predicted Class 0\n",
      "\n",
      "Review: As a former Erasmus student I enjoyed this film very much. It was so realistic and funny. It really picked up the spirit that exists among Erasmus students. I hope, many other students will follow thi ...\n",
      "Label: 1 = Positive\n",
      "Predicted Label: [0.886789] Predicted Class 1\n",
      "\n",
      "Review: My God, Ryan Gosling has made a lot of deep characters in his career, this is one of his wonderful acting jobs. For me this is a very deep movie, needs a lot of concentration, not because is difficult ...\n",
      "Label: 1 = Positive\n",
      "Predicted Label: [0.9104777] Predicted Class 1\n",
      "\n",
      "Review: This film just won the best film award at the Cleveland International Film Festival. It's American title apparently is Autumn Spring. The acting is superb. The story takes you into the life of an elde ...\n",
      "Label: 1 = Positive\n",
      "Predicted Label: [0.9375128] Predicted Class 1\n",
      "\n",
      "Review: The cast for this production of Rigoletto is excellent. Edita Gruberova sings Gilda magnificently and passionately. Luciano Pavarotti also sings splendidly. Vergara is a fine Maddalena; Fedora Barbier ...\n",
      "Label: 1 = Positive\n",
      "Predicted Label: [0.9308384] Predicted Class 1\n",
      "\n",
      "Review: As long as you keep in mind that the production of this movie was a copyright ploy, and not intended as a serious release, it is actually surprising how not absolutely horrible it is. I even liked the ...\n",
      "Label: 0 = Negative\n",
      "Predicted Label: [0.16021743] Predicted Class 0\n",
      "\n",
      "Review: Every great once in a while, you stumble upon a movie that exceeds even your wildest expectations. Given the IMDb rating of 4.0, I wasn't really expecting much with The Brotherhood of Satan. I hoped t ...\n",
      "Label: 1 = Positive\n",
      "Predicted Label: [0.0902083] Predicted Class 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for X_batch, y_batch in datasets[\"test\"].batch(10).take(1):\n",
    "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
    "        print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
    "        print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
    "        print(\"Predicted Label:\", y_pred[c], \"Predicted Class\", classes[c])\n",
    "        print()\n",
    "        c = c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
